{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of MPI in Devito\n",
    "\n",
    "Distributed-memory parallelism via MPI is designed so that users can \"think sequentially\" for as much as possi-\n",
    "ble. The few things requested to the user are:\n",
    "\n",
    "* Like any other MPI program, run with `mpirun -np X python ...`\n",
    "* Some pre- and/or post-processing may be rank-specific (e.g., we may want to plot on a given MPI rank only, even though this might be hidden away in the next Devito releases, when newer support APIs will be provided.\n",
    "* Parallel I/O (if and when necessary) to populate the MPI-distributed datasets in input to a Devito Operator. If a shared file system is available, there are a few simple alternatives to pick from, such as NumPy’s memory-mapped arrays.\n",
    "\n",
    "To enable MPI, users have two options. Either export the environment variable `DEVITO_MPI=1` or, programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import configuration\n",
    "configuration['mpi'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Operator` will then generate MPI code, including sends/receives for halo exchanges. Below, we introduce a running example through which we explain how domain decomposition as well as data access (read/write) and distribution work. Performance optimizations are discussed [in a later section](#Performance-optimizations).\n",
    "\n",
    "Let's start by creating a `TimeFunction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import Grid, TimeFunction, Eq, Operator \n",
    "grid = Grid(shape=(8, 8))\n",
    "u = TimeFunction(name=\"u\", grid=grid, space_order=2, time_order=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain decomposition is performed when creating `Grid(...)`. Users may supply their own domain decomposition, but this is not shown in this notebook. Devito exploits the MPI Cartesian topology abstraction to logically split the `Grid` over the available MPI processes. Since `u` is defined over a decomposed `Grid`, its data get distributed too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devito is strongly based on NumPy arrays -- `u.data` returns a `Data` object, which is a subclass of `numpy.ndarray`. The fact that `u.data` is (under the hood) distributed is completely abstracted away at the user level -- the perception is always to index into local arrays (i.e., classic NumPy arrays), even in the case of MPI execution. All sort of NumPy indexing schemes (basic, slicing, etc.) are supported. Underneath, proper global-to-local index conversion routines are used to propagate a read/write access to the impacted subset of ranks. For example, we can write into a slice-generated view of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.data[0, 2:-2, 2:-2] = 1.\n",
    "u.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only limitation, currently, is that a data access cannot require a direct data exchange among two or more processes (e.g., the assignment `u.data[0, 0] = u.data[7, 7]` will raise an exception unless both entries belong to the same MPI rank).\n",
    "\n",
    "We can finally write out a trivial `Operator` to try running something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.00 s\n"
     ]
    }
   ],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "op = Operator(Eq(u.forward, u + 1))\n",
    "summary = op.apply(time_M=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now check again the (distributed) content of our `u.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 2., 2., 2., 2., 1., 1.],\n",
       "       [1., 1., 2., 2., 2., 2., 1., 1.],\n",
       "       [1., 1., 2., 2., 2., 2., 1., 1.],\n",
       "       [1., 1., 2., 2., 2., 2., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything as expected. We could also peek at the generated code, because we may be curious to see what sort of MPI calls Devito has generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define _POSIX_C_SOURCE 200809L\n",
      "#include \"stdlib.h\"\n",
      "#include \"math.h\"\n",
      "#include \"sys/time.h\"\n",
      "#include \"mpi.h\"\n",
      "\n",
      "struct dataobj\n",
      "{\n",
      "  void *restrict data;\n",
      "  int * size;\n",
      "  int * npsize;\n",
      "  int * dsize;\n",
      "  int * hsize;\n",
      "  int * hofs;\n",
      "  int * oofs;\n",
      "} ;\n",
      "\n",
      "struct profiler\n",
      "{\n",
      "  double section0;\n",
      "} ;\n",
      "\n",
      "\n",
      "int Kernel(struct dataobj *restrict u_vec, const int time_M, const int time_m, struct profiler * timers, const int x_M, const int x_m, const int y_M, const int y_m)\n",
      "{\n",
      "  float (*restrict u)[u_vec->size[1]][u_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[u_vec->size[1]][u_vec->size[2]]) u_vec->data;\n",
      "  for (int time = time_m, t0 = (time)%(1), t1 = (time)%(1); time <= time_M; time += 1, t0 = (time)%(1), t1 = (time)%(1))\n",
      "  {\n",
      "    struct timeval start_section0, end_section0;\n",
      "    gettimeofday(&start_section0, NULL);\n",
      "    /* Begin section0 */\n",
      "    for (int x = x_m; x <= x_M; x += 1)\n",
      "    {\n",
      "      #pragma omp simd aligned(u:16)\n",
      "      for (int y = y_m; y <= y_M; y += 1)\n",
      "      {\n",
      "        u[t1][x + 2][y + 2] = u[t0][x + 2][y + 2] + 1;\n",
      "      }\n",
      "    }\n",
      "    /* End section0 */\n",
      "    gettimeofday(&end_section0, NULL);\n",
      "    timers->section0 += (double)(end_section0.tv_sec-start_section0.tv_sec)+(double)(end_section0.tv_usec-start_section0.tv_usec)/1000000;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on. There's nothing MPI-specific here! Apart from the header file `#include \"mpi.h\"`. What's going on? Well, it's simple. Devito was smart enough to realize that this trivial `Operator` doesn't even need any sort of halo exchange -- the `Eq` implements a pure \"map computation\" (i.e., fully parallel), so we can just let each MPI process do its job without any need for synchronization. We might want try again with a proper stencil `Eq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define _POSIX_C_SOURCE 200809L\n",
      "#include \"stdlib.h\"\n",
      "#include \"math.h\"\n",
      "#include \"sys/time.h\"\n",
      "#include \"mpi.h\"\n",
      "\n",
      "struct dataobj\n",
      "{\n",
      "  void *restrict data;\n",
      "  int * size;\n",
      "  int * npsize;\n",
      "  int * dsize;\n",
      "  int * hsize;\n",
      "  int * hofs;\n",
      "  int * oofs;\n",
      "} ;\n",
      "\n",
      "struct neighborhood\n",
      "{\n",
      "  int ll, lc, lr;\n",
      "  int cl, cc, cr;\n",
      "  int rl, rc, rr;\n",
      "} ;\n",
      "\n",
      "struct profiler\n",
      "{\n",
      "  double section0;\n",
      "} ;\n",
      "\n",
      "void haloupdate0(struct dataobj *restrict a_vec, MPI_Comm comm, struct neighborhood * nb, int otime);\n",
      "void sendrecv0(struct dataobj *restrict a_vec, const int buf_x_size, const int buf_y_size, int ogtime, int ogx, int ogy, int ostime, int osx, int osy, int fromrank, int torank, MPI_Comm comm);\n",
      "void gather0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy);\n",
      "void scatter0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy);\n",
      "\n",
      "int Kernel(const float h_x, struct dataobj *restrict u_vec, MPI_Comm comm, struct neighborhood * nb, const int time_M, const int time_m, struct profiler * timers, const int x_M, const int x_m, const int y_M, const int y_m)\n",
      "{\n",
      "  float (*restrict u)[u_vec->size[1]][u_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[u_vec->size[1]][u_vec->size[2]]) u_vec->data;\n",
      "  for (int time = time_m, t0 = (time)%(1), t1 = (time)%(1); time <= time_M; time += 1, t0 = (time)%(1), t1 = (time)%(1))\n",
      "  {\n",
      "    struct timeval start_section0, end_section0;\n",
      "    gettimeofday(&start_section0, NULL);\n",
      "    /* Begin section0 */\n",
      "    haloupdate0(u_vec,comm,nb,t0);\n",
      "    for (int x = x_m; x <= x_M; x += 1)\n",
      "    {\n",
      "      #pragma omp simd aligned(u:16)\n",
      "      for (int y = y_m; y <= y_M; y += 1)\n",
      "      {\n",
      "        float r0 = 1.0/h_x;\n",
      "        u[t1][x + 2][y + 2] = 5.0e-1F*(-r0*u[t0][x + 1][y + 2] + r0*u[t0][x + 3][y + 2]) + 1;\n",
      "      }\n",
      "    }\n",
      "    /* End section0 */\n",
      "    gettimeofday(&end_section0, NULL);\n",
      "    timers->section0 += (double)(end_section0.tv_sec-start_section0.tv_sec)+(double)(end_section0.tv_usec-start_section0.tv_usec)/1000000;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "void haloupdate0(struct dataobj *restrict a_vec, MPI_Comm comm, struct neighborhood * nb, int otime)\n",
      "{\n",
      "  sendrecv0(a_vec,a_vec->hsize[3],a_vec->npsize[2],otime,a_vec->oofs[2],a_vec->hofs[4],otime,a_vec->hofs[3],a_vec->hofs[4],nb->rc,nb->lc,comm);\n",
      "  sendrecv0(a_vec,a_vec->hsize[2],a_vec->npsize[2],otime,a_vec->oofs[3],a_vec->hofs[4],otime,a_vec->hofs[2],a_vec->hofs[4],nb->lc,nb->rc,comm);\n",
      "}\n",
      "\n",
      "void sendrecv0(struct dataobj *restrict a_vec, const int buf_x_size, const int buf_y_size, int ogtime, int ogx, int ogy, int ostime, int osx, int osy, int fromrank, int torank, MPI_Comm comm)\n",
      "{\n",
      "  float (*bufs)[buf_y_size];\n",
      "  float (*bufg)[buf_y_size];\n",
      "  posix_memalign((void**)&bufs, 64, sizeof(float[buf_x_size][buf_y_size]));\n",
      "  posix_memalign((void**)&bufg, 64, sizeof(float[buf_x_size][buf_y_size]));\n",
      "  MPI_Request rrecv;\n",
      "  MPI_Request rsend;\n",
      "  MPI_Irecv((float *)bufs,buf_x_size*buf_y_size,MPI_FLOAT,fromrank,13,comm,&rrecv);\n",
      "  if (torank != MPI_PROC_NULL)\n",
      "  {\n",
      "    gather0((float *)bufg,buf_x_size,buf_y_size,a_vec,ogtime,ogx,ogy);\n",
      "  }\n",
      "  MPI_Isend((float *)bufg,buf_x_size*buf_y_size,MPI_FLOAT,torank,13,comm,&rsend);\n",
      "  MPI_Wait(&rsend,MPI_STATUS_IGNORE);\n",
      "  MPI_Wait(&rrecv,MPI_STATUS_IGNORE);\n",
      "  if (fromrank != MPI_PROC_NULL)\n",
      "  {\n",
      "    scatter0((float *)bufs,buf_x_size,buf_y_size,a_vec,ostime,osx,osy);\n",
      "  }\n",
      "  free(bufs);\n",
      "  free(bufg);\n",
      "}\n",
      "\n",
      "void gather0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy)\n",
      "{\n",
      "  float (*restrict buf)[buf_y_size] __attribute__ ((aligned (64))) = (float (*)[buf_y_size]) buf_vec;\n",
      "  float (*restrict a)[a_vec->size[1]][a_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[a_vec->size[1]][a_vec->size[2]]) a_vec->data;\n",
      "  for (int x = 0; x <= buf_x_size - 1; x += 1)\n",
      "  {\n",
      "    for (int y = 0; y <= buf_y_size - 1; y += 1)\n",
      "    {\n",
      "      buf[x][y] = a[otime][x + ox][y + oy];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "void scatter0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy)\n",
      "{\n",
      "  float (*restrict buf)[buf_y_size] __attribute__ ((aligned (64))) = (float (*)[buf_y_size]) buf_vec;\n",
      "  float (*restrict a)[a_vec->size[1]][a_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[a_vec->size[1]][a_vec->size[2]]) a_vec->data;\n",
      "  for (int x = 0; x <= buf_x_size - 1; x += 1)\n",
      "  {\n",
      "    for (int y = 0; y <= buf_y_size - 1; y += 1)\n",
      "    {\n",
      "      a[otime][x + ox][y + oy] = buf[x][y];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op = Operator(Eq(u.forward, u.dx + 1))\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh -- not it does look a bit more complicated than before, though it still is pretty much human-readable, if we pay careful attention. The `haloupdate0` routine performs a blocking halo exchange, gathering boundary data into a contiguous buffer (`gather0`), sending it to one or more neighboring processes (`sendrecv0`), receiving new data from one or more neighboring processes (`sendrecv0`), and finally scattering the received data at the right array locations (`scatter0`). But this is just the beginning: no performance optimizations are applied here. We'll see more later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at other scenarios and performance optimizations, there is one last thing it worth discussing -- the `data_with_halo` view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 2., 2., 2., 2., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 2., 2., 2., 2., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 2., 2., 2., 2., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 2., 2., 2., 2., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.data_with_halo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again a global data view. The shown _with_halo_ is the \"true\" halo surrounding the physical domain, **not** the halo used for the MPI halo exchanges (often referred to as \"ghost region\"). So it gets trivial for a user to initialize the \"true\" halo region (which is typically read by a stencil `Eq` when an `Operator` iterates in proximity of the domain bounday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.data_with_halo[:] = 1.\n",
    "u.data_with_halo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and SparseFunction\n",
    "\n",
    "A `SparseFunction` represents a sparse set of points which are generically unaligned with the `Grid`. A sparse point could be anywhere within a grid, and is therefore attached some coordinates. Given a sparse point, Devito looks at its coordinates and, based on the domain decomposition, **logically** assigns it to a given MPI process; this is purely logical ownership, as in Python-land, before running an Operator, the sparse point physically lives on the MPI rank which created it. Within `op.apply`, right before jumping to C-land, the sparse points are scattered to their logical owners; upon returning to Python-land, the sparse points are gathered back to their original location.\n",
    "\n",
    "In the following example, we attempt injection of four sparse points into the neighboring grid points via linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devito import Function, SparseFunction\n",
    "grid = Grid(shape=(4, 4), extent=(3.0, 3.0))\n",
    "x, y = grid.dimensions\n",
    "f = Function(name='f', grid=grid)\n",
    "coords = [(0.5, 0.5), (1.5, 2.5), (1.5, 1.5), (2.5, 1.5)]\n",
    "sf = SparseFunction(name='sf', grid=grid, npoint=len(coords), coordinates=coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "* O be a grid point\n",
    "* x be a halo point\n",
    "* A, B, C, D be the sparse points\n",
    "\n",
    "We show the global view, that is what the user \"sees\".\n",
    "\n",
    "```\n",
    "O --- O --- O --- O\n",
    "|  A  |     |     |\n",
    "O --- O --- O --- O\n",
    "|     |  C  |  B  |\n",
    "O --- O --- O --- O\n",
    "|     |  D  |     |\n",
    "O --- O --- O --- O\n",
    "```\n",
    "\n",
    "And now the local view, that is what the MPI ranks own when jumping to C-land.        \n",
    "\n",
    "```\n",
    "Rank 0          Rank 1\n",
    "O --- O --- x   x --- O --- O\n",
    "|  A  |     |   |     |     |\n",
    "O --- O --- x   x --- O --- O\n",
    "|     |  C  |   |  C  |  B  |\n",
    "x --- x --- x   x --- x --- x\n",
    "\n",
    "Rank 2           Rank 3\n",
    "x --- x --- x   x --- x --- x\n",
    "|     |  C  |   |  C  |  B  |\n",
    "O --- O --- x   x --- O --- O\n",
    "|     |  D  |   |  D  |     |\n",
    "O --- O --- x   x --- O --- O\n",
    "```\n",
    "\n",
    "We observe that the sparse points along the boundary of two or more MPI ranks are _duplicated_ and thus redundantly computed over multiple processes. However, the contributions from these points to the neighboring halo points are naturally ditched, so the final result of the interpolation is as expected. Let's convince ourselves that this is the case. We assign a value of $4$ to each sparse point. Since we are using linear interpolation and all points are placed at the exact center of a grid quadrant, we expect that the contribution of each sparse point to a neighboring grid point will be $4 * 0.25 = 1.25$. Based on the global view above, we eventually expect `f` to look like as follows:\n",
    "\n",
    "```\n",
    "1.25 --- 1.25 --- 0.00 --- 0.00\n",
    "|         |        |        |\n",
    "1.25 --- 2.50 --- 2.50 --- 1.25\n",
    "|         |        |        |\n",
    "0.00 --- 2.50 --- 3.75 --- 1.25\n",
    "|         |        |        |\n",
    "0.00 --- 1.25 --- 1.25 --- 0.00\n",
    "```\n",
    "\n",
    "Let's check this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.00 s\n"
     ]
    }
   ],
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "sf.data[:] = 4.\n",
    "op = Operator(sf.inject(field=f, expr=sf + 1))\n",
    "summary = op.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[1.25, 1.25, 0.  , 0.  ],\n",
       "      [1.25, 2.5 , 2.5 , 1.25],\n",
       "      [0.  , 2.5 , 3.75, 1.25],\n",
       "      [0.  , 1.25, 1.25, 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance optimizations\n",
    "\n",
    "The Devito compiler applies several optimizations before generating code.\n",
    "\n",
    "* Redundant halo exchanges are identified and removed. A halo exchange is redundant if a prior halo exchange carries out the same `Function` update and the data is not “dirty” yet.\n",
    "* Computation/communication overlap, with explicit prodding of the asynchronous progress engine to make sure that non-blocking communications execute in background during the compute part.\n",
    "* Halo exchanges could also be reshuffled to maximize the extension of the computation/communication overlap region.\n",
    "\n",
    "To run with all these optimizations enabled, instead of `DEVITO_MPI=1`, users should set `DEVITO_MPI=full`, or, equivalently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration['mpi'] = 'full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now peek at the generated code to see that things now look differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define _POSIX_C_SOURCE 200809L\n",
      "#include \"stdlib.h\"\n",
      "#include \"math.h\"\n",
      "#include \"sys/time.h\"\n",
      "#include \"mpi.h\"\n",
      "\n",
      "struct dataobj\n",
      "{\n",
      "  void *restrict data;\n",
      "  int * size;\n",
      "  int * npsize;\n",
      "  int * dsize;\n",
      "  int * hsize;\n",
      "  int * hofs;\n",
      "  int * oofs;\n",
      "} ;\n",
      "\n",
      "struct msg\n",
      "{\n",
      "  int * ofss;\n",
      "  int * ofsg;\n",
      "  int fromrank;\n",
      "  int torank;\n",
      "  void * bufs;\n",
      "  void * bufg;\n",
      "  int * sizes;\n",
      "  MPI_Request rrecv;\n",
      "  MPI_Request rsend;\n",
      "} ;\n",
      "\n",
      "struct profiler\n",
      "{\n",
      "  double section0;\n",
      "} ;\n",
      "\n",
      "struct region\n",
      "{\n",
      "  int x_m;\n",
      "  int x_M;\n",
      "  int y_m;\n",
      "  int y_M;\n",
      "} ;\n",
      "\n",
      "void haloupdate0(struct dataobj *restrict a_vec, MPI_Comm comm, struct msg * msg0_0, int otime);\n",
      "void gather0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy);\n",
      "void halowait0(struct dataobj *restrict a_vec, int otime, struct msg * msg0_0);\n",
      "void scatter0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy);\n",
      "void pokempi0(struct msg * msg0_0);\n",
      "void compute0(const float h_x, struct dataobj *restrict u_vec, struct msg * msg0_0, const int t0, const int t1, const int x_M, const int x_m, const int y_M, const int y_m);\n",
      "void remainder0(struct dataobj *restrict u_vec, const float h_x, struct msg * msg0_0, struct region * reg0, const int t0, const int t1);\n",
      "\n",
      "int Kernel(struct dataobj *restrict u_vec, MPI_Comm comm, const float h_x, struct msg * msg0_0, struct region * reg0, const int time_M, const int time_m, struct profiler * timers, const int x_M, const int x_m, const int y_M, const int y_m)\n",
      "{\n",
      "  for (int time = time_m, t0 = (time)%(1), t1 = (time)%(1); time <= time_M; time += 1, t0 = (time)%(1), t1 = (time)%(1))\n",
      "  {\n",
      "    struct timeval start_section0, end_section0;\n",
      "    gettimeofday(&start_section0, NULL);\n",
      "    /* Begin section0 */\n",
      "    haloupdate0(u_vec,comm,msg0_0,t0);\n",
      "    compute0(h_x,u_vec,msg0_0,t0,t1,x_M - 2,x_m + 2,y_M - 2,y_m + 2);\n",
      "    halowait0(u_vec,t0,msg0_0);\n",
      "    remainder0(u_vec,h_x,msg0_0,reg0,t0,t1);\n",
      "    /* End section0 */\n",
      "    gettimeofday(&end_section0, NULL);\n",
      "    timers->section0 += (double)(end_section0.tv_sec-start_section0.tv_sec)+(double)(end_section0.tv_usec-start_section0.tv_usec)/1000000;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "void haloupdate0(struct dataobj *restrict a_vec, MPI_Comm comm, struct msg * msg0_0, int otime)\n",
      "{\n",
      "  for (int i = 0; i <= 1; i += 1)\n",
      "  {\n",
      "    MPI_Irecv(msg0_0[i].bufs,msg0_0[i].sizes[0]*msg0_0[i].sizes[1],MPI_FLOAT,msg0_0[i].fromrank,13,comm,&(msg0_0[i].rrecv));\n",
      "    if (msg0_0[i].torank != MPI_PROC_NULL)\n",
      "    {\n",
      "      gather0(msg0_0[i].bufg,msg0_0[i].sizes[0],msg0_0[i].sizes[1],a_vec,otime,msg0_0[i].ofsg[0],msg0_0[i].ofsg[1]);\n",
      "    }\n",
      "    MPI_Isend(msg0_0[i].bufg,msg0_0[i].sizes[0]*msg0_0[i].sizes[1],MPI_FLOAT,msg0_0[i].torank,13,comm,&(msg0_0[i].rsend));\n",
      "  }\n",
      "}\n",
      "\n",
      "void gather0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy)\n",
      "{\n",
      "  float (*restrict buf)[buf_y_size] __attribute__ ((aligned (64))) = (float (*)[buf_y_size]) buf_vec;\n",
      "  float (*restrict a)[a_vec->size[1]][a_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[a_vec->size[1]][a_vec->size[2]]) a_vec->data;\n",
      "  for (int x = 0; x <= buf_x_size - 1; x += 1)\n",
      "  {\n",
      "    for (int y = 0; y <= buf_y_size - 1; y += 1)\n",
      "    {\n",
      "      buf[x][y] = a[otime][x + ox][y + oy];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "void halowait0(struct dataobj *restrict a_vec, int otime, struct msg * msg0_0)\n",
      "{\n",
      "  for (int i = 0; i <= 1; i += 1)\n",
      "  {\n",
      "    MPI_Wait(&(msg0_0[i].rsend),MPI_STATUS_IGNORE);\n",
      "    MPI_Wait(&(msg0_0[i].rrecv),MPI_STATUS_IGNORE);\n",
      "    if (msg0_0[i].fromrank != MPI_PROC_NULL)\n",
      "    {\n",
      "      scatter0(msg0_0[i].bufs,msg0_0[i].sizes[0],msg0_0[i].sizes[1],a_vec,otime,msg0_0[i].ofss[0],msg0_0[i].ofss[1]);\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "void scatter0(float *restrict buf_vec, const int buf_x_size, const int buf_y_size, struct dataobj *restrict a_vec, int otime, int ox, int oy)\n",
      "{\n",
      "  float (*restrict buf)[buf_y_size] __attribute__ ((aligned (64))) = (float (*)[buf_y_size]) buf_vec;\n",
      "  float (*restrict a)[a_vec->size[1]][a_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[a_vec->size[1]][a_vec->size[2]]) a_vec->data;\n",
      "  for (int x = 0; x <= buf_x_size - 1; x += 1)\n",
      "  {\n",
      "    for (int y = 0; y <= buf_y_size - 1; y += 1)\n",
      "    {\n",
      "      a[otime][x + ox][y + oy] = buf[x][y];\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "void pokempi0(struct msg * msg0_0)\n",
      "{\n",
      "  int flag = 0;\n",
      "  for (int i = 0; i <= 1; i += 1)\n",
      "  {\n",
      "    MPI_Test(&(msg0_0[i].rsend),&flag,MPI_STATUS_IGNORE);\n",
      "    MPI_Test(&(msg0_0[i].rrecv),&flag,MPI_STATUS_IGNORE);\n",
      "  }\n",
      "}\n",
      "\n",
      "void compute0(const float h_x, struct dataobj *restrict u_vec, struct msg * msg0_0, const int t0, const int t1, const int x_M, const int x_m, const int y_M, const int y_m)\n",
      "{\n",
      "  float (*restrict u)[u_vec->size[1]][u_vec->size[2]] __attribute__ ((aligned (64))) = (float (*)[u_vec->size[1]][u_vec->size[2]]) u_vec->data;\n",
      "  for (int x = x_m; x <= x_M; x += 1)\n",
      "  {\n",
      "    pokempi0(msg0_0);\n",
      "    #pragma omp simd aligned(u:16)\n",
      "    for (int y = y_m; y <= y_M; y += 1)\n",
      "    {\n",
      "      float r0 = 1.0/h_x;\n",
      "      u[t1][x + 2][y + 2] = 5.0e-1F*(-r0*u[t0][x + 1][y + 2] + r0*u[t0][x + 3][y + 2]) + 1;\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "void remainder0(struct dataobj *restrict u_vec, const float h_x, struct msg * msg0_0, struct region * reg0, const int t0, const int t1)\n",
      "{\n",
      "  for (int i = 0; i <= 7; i += 1)\n",
      "  {\n",
      "    compute0(h_x,u_vec,msg0_0,t0,t1,reg0[i].x_M,reg0[i].x_m,reg0[i].y_M,reg0[i].y_m);\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op = Operator(Eq(u.forward, u.dx + 1))\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of the time-stepping loop has changed, as it now implements a classic computation/communication overlap scheme:\n",
    "\n",
    "* `haloupdate0` triggers non-blocking communications;\n",
    "* `compute0` executes the core domain region, that is the sub-region which doesn't require reading from halo data to be computed;\n",
    "* `halowait0` wait and terminates the non-blocking communications;\n",
    "* `remainder0`, which internally calls `compute0`, computes the boundary region requiring the now up-to-date halo data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devito]",
   "language": "python",
   "name": "conda-env-devito-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
