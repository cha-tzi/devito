{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple backpropagation in Devito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll implement a simple convolutional neural network (CNN) in Devito, run a forward pass through it and then use backpropagation to obtain gradients necessary for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN will have the following structure:\n",
    "1. Max pool layer: input size 1x2x4x4, kernel size 2x2, stride 1x1\n",
    "2. Convolutional layer: input size 1x2x3x3, kernel size 2x2x2, stride 1x1, activation ReLU\n",
    "3. Flattening layer\n",
    "4. Fully connected layer: input size 8x1, kernel size 3x8, activation softmax\n",
    "\n",
    "*Size glossary: batch size x channels x height x width **or** output channels x height x width **or** height x width. Height and width are equivalent to rows and columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All parameters for the forward pass will be (pseudo)random numbers generated by `np.random.rand`. Therefore, different results will be obtained each time the notebook is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devito.ml\n",
    "import numpy as np\n",
    "from sympy import Max\n",
    "from sympy.functions import sign\n",
    "from devito import Operator, Eq, Inc\n",
    "from devito.ml.loss import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected_results):\n",
    "    layer1 = devito.ml.Subsampling(kernel_size=(2, 2),\n",
    "                                   input_size=(1, 2, 4, 4),\n",
    "                                   function=lambda l: Max(*l),\n",
    "                                   generate_code=False)\n",
    "    layer2 = devito.ml.Conv(kernel_size=(2, 2, 2),\n",
    "                            input_size=(1, 2, 3, 3),\n",
    "                            activation=lambda x: Max(0, x),\n",
    "                            generate_code=False)\n",
    "    layer_flat = devito.ml.Flat(input_size=(1, 2, 2, 2),\n",
    "                                generate_code=False)\n",
    "    layer3 = devito.ml.FullyConnectedSoftmax(weight_size=(3, 8),\n",
    "                                             input_size=(8, 1),\n",
    "                                             generate_code=False)\n",
    "    \n",
    "    eqs = layer1.equations() + layer2.equations(layer1.result) + \\\n",
    "            layer_flat.equations(layer2.result) + layer3.equations(layer_flat.result)\n",
    "    \n",
    "    op = Operator(eqs)\n",
    "    \n",
    "    layer2.kernel.data[:] = conv_kernel\n",
    "    layer2.bias.data[:] = conv_bias\n",
    "    \n",
    "    layer3.kernel.data[:] = fc_weights\n",
    "    layer3.bias.data[:] = fc_bias\n",
    "    \n",
    "    layer1.input.data[:] = input_data\n",
    "    \n",
    "    op.apply()\n",
    "    \n",
    "    gradients = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        result = layer3.result.data[i]\n",
    "        if i == expected_results[0]:\n",
    "            result -= 1\n",
    "        gradients.append(result)\n",
    "    \n",
    "    layer3.result_gradients.data[:] = gradients\n",
    "    \n",
    "    dims = [layer3.bias_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[0],\n",
    "            layer3.kernel_gradients.dimensions[1],\n",
    "            layer_flat.result_gradients.dimensions[0],\n",
    "            layer3.kernel.dimensions[1],\n",
    "            layer3.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[0],\n",
    "            layer2.result_gradients.dimensions[1],\n",
    "            layer2.result_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[0],\n",
    "            layer2.kernel_gradients.dimensions[1],\n",
    "            layer2.kernel_gradients.dimensions[2],\n",
    "            layer2.kernel_gradients.dimensions[3],\n",
    "            layer2.bias_gradients.dimensions[0]]\n",
    "    \n",
    "    _, _, layer2_height, layer2_width = layer2.kernel.shape\n",
    "    \n",
    "    backprop_eqs = [\n",
    "        Eq(layer3.bias_gradients[dims[0]], layer3.result_gradients[dims[0]]),\n",
    "        Eq(layer3.kernel_gradients[dims[1], dims[2]],\n",
    "           layer_flat.result[dims[2], 0] * layer3.result_gradients[dims[1]]),\n",
    "        Inc(layer_flat.result_gradients[dims[4]],\n",
    "            layer3.kernel[dims[5], dims[4]] * layer3.result_gradients[dims[5]]),\n",
    "        Eq(layer_flat.result_gradients[dims[3]],\n",
    "           layer_flat.result_gradients[dims[3]] * sign(layer_flat.result[dims[3], 0])),\n",
    "        Eq(layer2.result_gradients[dims[6], dims[7], dims[8]],\n",
    "           layer_flat.result_gradients[dims[6] * layer2_height * layer2_width + dims[7] * layer2_height + dims[8]]),\n",
    "        Inc(layer2.bias_gradients[dims[13]], layer2.result_gradients[dims[13], dims[7], dims[8]]),\n",
    "        Eq(layer2.kernel_gradients[dims[9], dims[10], dims[11], dims[12]],\n",
    "            sum([layer2.result_gradients[dims[9], x, y] * layer1.result[0, dims[10], dims[11] + x, dims[12] + y]\n",
    "                 for x in range(layer2.result_gradients.shape[1])\n",
    "                 for y in range(layer2.result_gradients.shape[2])]))\n",
    "    ]\n",
    "    \n",
    "    backprop_op = Operator(backprop_eqs)\n",
    "    backprop_op.apply()\n",
    "    \n",
    "    return (layer2.kernel_gradients.data, layer2.bias_gradients.data,\n",
    "            layer3.kernel_gradients.data, layer3.bias_gradients.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel = np.random.rand(2, 2, 2)\n",
    "conv_bias = np.random.rand(2)\n",
    "\n",
    "fc_weights = np.random.rand(3, 8)\n",
    "fc_bias = np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[[[1, 2, 3, 4],\n",
    "                         [5, 6, 7, 8],\n",
    "                         [9, 10, 11, 12],\n",
    "                         [13, 14, 15, 16]],\n",
    "                        [[-1, -2, 0, 1],\n",
    "                         [-2, -3, 1, 2],\n",
    "                         [3, 4, 2, -1],\n",
    "                         [-2, -3, -4, 9]]]],\n",
    "                     dtype=np.float64)\n",
    "expected = np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksymilian/Desktop/UROP/devito/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n",
      "Operator `Kernel` run in 0.01 s\n",
      "Operator `Kernel` run in 0.01 s\n"
     ]
    }
   ],
   "source": [
    "conv_kernel_grad, conv_bias_grad, fc_kernel_grad, fc_bias_grad = backward_pass(conv_kernel, conv_bias, fc_weights, fc_bias, input_data, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.00218138  0.00241714]\n",
      "   [ 0.0031244   0.00336016]]\n",
      "\n",
      "  [[ 0.00072036  0.00120074]\n",
      "   [ 0.00094302  0.0006705 ]]]\n",
      "\n",
      "\n",
      " [[[-0.00361136 -0.00397412]\n",
      "   [-0.00506242 -0.00542519]]\n",
      "\n",
      "  [[-0.00156334 -0.00158768]\n",
      "   [-0.00145106 -0.00102258]]]]\n"
     ]
    }
   ],
   "source": [
    "print(conv_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00023576 -0.00036277]\n"
     ]
    }
   ],
   "source": [
    "print(conv_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are kernel gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0117087   0.01286574  0.01864753  0.02136425  0.01155483  0.01271187\n",
      "   0.01849366  0.02121037]\n",
      " [ 0.00014577  0.00016017  0.00023216  0.00026598  0.00014385  0.00015826\n",
      "   0.00023024  0.00026406]\n",
      " [-0.01185447 -0.01302592 -0.01887969 -0.02163022 -0.01169869 -0.01287013\n",
      "  -0.0187239  -0.02147444]]\n"
     ]
    }
   ],
   "source": [
    "print(fc_kernel_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are bias gradients of the fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.71736625e-04  7.11793346e-06 -5.78854559e-04]\n"
     ]
    }
   ],
   "source": [
    "print(fc_bias_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with PyTorch\n",
    "To check correctness, we will carry out the same activities using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 2, 2)\n",
    "        self.fc = nn.Linear(8, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(x, 2, stride=(1, 1))\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.conv.weight[:] = torch.from_numpy(conv_kernel)\n",
    "    net.conv.bias[:] = torch.from_numpy(conv_bias)\n",
    "    \n",
    "    net.fc.weight[:] = torch.from_numpy(fc_weights)\n",
    "    net.fc.bias[:] = torch.from_numpy(fc_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "input_data_tensor = torch.from_numpy(input_data)\n",
    "outputs = net(input_data_tensor)\n",
    "net.zero_grad()\n",
    "loss = criterion(outputs, torch.from_numpy(expected))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[7.43549969e-14 7.57149188e-14]\n",
      "   [7.66200505e-14 7.74393931e-14]]\n",
      "\n",
      "  [[4.92160349e-14 3.77430851e-14]\n",
      "   [8.20894858e-14 1.32594429e-13]]]\n",
      "\n",
      "\n",
      " [[[2.93015010e-14 2.99005854e-14]\n",
      "   [3.10113502e-14 3.13358661e-14]]\n",
      "\n",
      "  [[1.28994085e-14 1.40674657e-14]\n",
      "   [3.52667596e-14 6.55240783e-14]]]]\n",
      "1.325944291681414e-13\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_kernel_grad = net.conv.weight.grad.numpy()\n",
    "conv_kernel_error = abs(conv_kernel_grad - pytorch_conv_kernel_grad) / abs(pytorch_conv_kernel_grad)\n",
    "print(conv_kernel_error)\n",
    "print(np.amax(conv_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative convolutional layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.22044571e-14 3.52667596e-14]\n",
      "8.220445707807844e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_conv_bias_grad = net.conv.bias.grad.numpy()\n",
    "conv_bias_error = abs(conv_bias_grad - pytorch_conv_bias_grad) / abs(pytorch_conv_bias_grad)\n",
    "print(conv_bias_error)\n",
    "print(np.amax(conv_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer kernel error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.42230481e-14 1.42922693e-14 1.39540472e-14 1.41283662e-14\n",
      "  1.41121910e-14 1.43288052e-14 1.42577496e-14 1.42308606e-14]\n",
      " [1.41317893e-14 1.38762238e-14 1.42439643e-14 1.40631917e-14\n",
      "  1.39431348e-14 1.38729178e-14 1.40093012e-14 1.39599203e-14]\n",
      " [0.00000000e+00 2.66349495e-16 1.83766135e-16 0.00000000e+00\n",
      "  0.00000000e+00 1.34786761e-16 0.00000000e+00 0.00000000e+00]]\n",
      "1.4328805210639554e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_kernel_grad = net.fc.weight.grad.numpy()\n",
    "fc_kernel_error = abs(fc_kernel_grad - pytorch_fc_kernel_grad) / abs(pytorch_fc_kernel_grad)\n",
    "print(fc_kernel_error)\n",
    "print(np.amax(fc_kernel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the relative fully connected layer bias error along with the maximum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.42224863e-14 1.40419812e-14 0.00000000e+00]\n",
      "1.4222486250765177e-14\n"
     ]
    }
   ],
   "source": [
    "pytorch_fc_bias_grad = net.fc.bias.grad.numpy()\n",
    "fc_bias_error = abs(fc_bias_grad - pytorch_fc_bias_grad) / abs(pytorch_fc_bias_grad)\n",
    "print(fc_bias_error)\n",
    "print(np.amax(fc_bias_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the maximum overall error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.325944291681414e-13\n"
     ]
    }
   ],
   "source": [
    "print(max(np.amax(conv_kernel_error), np.amax(conv_bias_error), np.amax(fc_kernel_error), np.amax(fc_bias_error)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
