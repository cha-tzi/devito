{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opperations with 4-axes tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/convolution_graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning the filter needs to have the same amount of channels as the input image. Each filter will result in one output channel. In other words, when a kernel is applied to an RGB (or an image with Multiple channels) then effectively sum up the output matrices (along with a bias terms) to yield a single-channel feature map. The number of channels of the output image will be equal to the number of filters applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch works on input tensors whose shape correspands to :\n",
    "\n",
    "**(batch_size,  num_input_channels,  image_height,  image_width)**\n",
    "\n",
    "Kernels must be tensors of size: \n",
    "\n",
    "\n",
    "**(out_channels,  in_channels,  kernel_height,  kernel_width )**\n",
    "\n",
    "out_channels: number of kernels, in_channels: number of channels in one kernel\n",
    "\n",
    "Arguments of torch.nn.Conv2d\n",
    "\n",
    "**(in_channel,  out_channel,  ker_size)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz/cifar-10-python.tar.gz to C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "#do the standard imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ## uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms.\n",
    "    torch.backends.cudnn.enabled   = False  ## does not enable the inbuilt cudnn deep learning library for training neural networks\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available(): ## run on GPUs if available\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "#Use torchvision.datasets.CIFAR10 to load the CIFAR10 dataset\n",
    "cifar10_train = CIFAR10(\"C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz\", download=True, train=True)\n",
    "#cifar10_test = CIFAR10(\"./\", download=True, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the dataset to a torch tensor\n",
    "X_train = torch.tensor(cifar10_train.data).float()\n",
    "#take only the first 50 images \n",
    "#and reshape to batchsize, channels, height, width\n",
    "small_tensor = X_train[0:50,:].transpose(3,1).transpose(2,3)\n",
    "single_image = small_tensor[0:1]\n",
    "single_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the kernel\n",
    "#out_channels, in_channels, kernel_height, kernel_width )\n",
    "#out_channels: number of kernels, in_channels: number of channels in one kernel\n",
    "SOBEL_VERTICAL = [[1, 0, -1],\n",
    "                  [2, 0, -2],\n",
    "                  [1, 0, -1]]\n",
    "SOBEL_HORIZONTAL = [[1, 2, 1],\n",
    "                    [0, 0, 0],\n",
    "                    [-1, -2, -1]]\n",
    "PREWITT_VERTICAL = [[1, 0, -1],\n",
    "                    [1, 0, -1],\n",
    "                    [1, 0, -1]]\n",
    "single_image = small_tensor[0:1,:]\n",
    "#single image with 4 axes\n",
    "print(single_image.shape)\n",
    "#create a kernel with 4 axes\n",
    "kernel_3d = torch.tensor([SOBEL_VERTICAL, SOBEL_HORIZONTAL, PREWITT_VERTICAL]).float()\n",
    "kernel_3d.unsqueeze_(0)\n",
    "kernel_3d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conop = nn.Conv2d(3,1,3, bias = False)\n",
    "input = single_image\n",
    "#set our own weights\n",
    "conop.weight = torch.nn.Parameter( kernel_3d )\n",
    "output = conop(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devito set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from devito import Operator, Function\n",
    "from numpy import array\n",
    "from devito import Grid, Function, dimensions, Eq, Inc\n",
    "import sympy\n",
    "class Layer(ABC):\n",
    "    def __init__(self, input_data):\n",
    "        self._input_data = input_data\n",
    "        self._R = self._allocate()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _allocate(self) -> Function:\n",
    "        # This method should return a Function object corresponding to\n",
    "        # an output of the layer.\n",
    "        pass\n",
    "\n",
    "    def execute(self) -> (Operator, array):\n",
    "        op = Operator(self.equations())\n",
    "        op.cfunction\n",
    "\n",
    "        return (op, self._R.data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def equations(self) -> list:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Dimentional Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0), shape=(31, 31), dimensions=(a, b)]\n",
      "b shape (32, 32)\n",
      "rhs Max(B[x, y], B[x, y + 1], B[x + 1, y], B[x + 1, y + 1])\n",
      "Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31, 31)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Subsampling(Layer):\n",
    "    def __init__(self, kernel_size, feature_map, function,\n",
    "                 stride=(1, 1), padding=(0, 0), activation=None,\n",
    "                 bias=0):\n",
    "        # All sizes are expressed as (rows, columns).\n",
    "\n",
    "        self._error_check(kernel_size, feature_map, stride, padding)\n",
    "\n",
    "        self._kernel_size = kernel_size\n",
    "        self._function = function\n",
    "        self._activation = activation\n",
    "        self._bias = bias\n",
    "\n",
    "        self._stride = stride\n",
    "        self._padding = padding\n",
    "\n",
    "        super().__init__(input_data=feature_map)\n",
    "\n",
    "    def _error_check(self, kernel_size, feature_map, stride, padding):\n",
    "        if feature_map is None or len(feature_map) == 0:\n",
    "            raise Exception(\"Feature map must not be empty\")\n",
    "\n",
    "        if kernel_size is None or len(kernel_size) != 2:\n",
    "            raise Exception(\"Kernel size is incorrect\")\n",
    "\n",
    "        if stride is None or len(stride) != 2:\n",
    "            raise Exception(\"Stride is incorrect\")\n",
    "\n",
    "        if stride[0] < 1 or stride[1] < 1:\n",
    "            raise Exception(\"Stride cannot be less than 1\")\n",
    "\n",
    "        if padding is None or len(padding) != 2:\n",
    "            raise Exception(\"Padding is incorrect\")\n",
    "\n",
    "        if padding[0] < 0 or padding[1] < 0:\n",
    "            raise Exception(\"Padding cannot be negative\")\n",
    "\n",
    "        map_height = len(feature_map) + 2 * padding[0]\n",
    "        map_width = len(feature_map[0]) + 2 * padding[1]\n",
    "        kernel_height, kernel_width = kernel_size\n",
    "\n",
    "        if (map_height - kernel_height) % stride[0] != 0 or \\\n",
    "           (map_width - kernel_width) % stride[1] != 0:\n",
    "            raise Exception(\"Stride \" + str(stride) + \" is not \"\n",
    "                            \"compatible with feature map, kernel and padding \"\n",
    "                            \"sizes\")\n",
    "\n",
    "    def _allocate(self):\n",
    "        map_height = len(self._input_data) + 2 * self._padding[0]\n",
    "        map_width = len(self._input_data[0]) + 2 * self._padding[1]\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "\n",
    "        gridB = Grid(shape=(map_height, map_width))\n",
    "        B = Function(name='B', grid=gridB, space_order=0)\n",
    "\n",
    "        a, b = dimensions('a b')\n",
    "        gridR = Grid(shape=((map_height - kernel_height + self._stride[0])\n",
    "                            // self._stride[0],\n",
    "                            (map_width - kernel_width + self._stride[1])\n",
    "                            // self._stride[1]),\n",
    "                     dimensions=(a, b))\n",
    "        R = Function(name='R', grid=gridR, space_order=0)\n",
    "\n",
    "        for i in range(self._padding[0], map_height - self._padding[0]):\n",
    "            B.data[i] = \\\n",
    "                np.concatenate(([0] * self._padding[1],\n",
    "                                self._input_data[i - self._padding[0]],\n",
    "                                [0] * self._padding[1]))\n",
    "\n",
    "        self._B = B\n",
    "        return R\n",
    "\n",
    "    def equations(self):\n",
    "        a, b = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "\n",
    "        rhs = self._function([self._B[self._stride[0] * a + i,\n",
    "                                      self._stride[1] * b + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width)]) + self._bias\n",
    "        print(\"rhs:\",rhs)\n",
    "        print(type(rhs))\n",
    "        if self._activation is not None:\n",
    "            rhs = self._activation(rhs)\n",
    "\n",
    "        return [Eq(self._R[a, b], rhs)]\n",
    "                \n",
    "Sample_obj = Subsampling((2,2),single_image[0][0], lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tupC = Sample_obj.execute()\n",
    "tupC[0].apply()\n",
    "tupC[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Eq(R[x, y], Max(B[x, y], B[x, y + 1], B[x + 1, y], B[x + 1, y + 1]))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj.equations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equations(self):\n",
    "        a, b, c, d = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        images = self._input_data.shape[0]\n",
    "        channels = self._input_data.shape[1] \n",
    "        \n",
    "\n",
    "        rhs = ([self._function(self._B[image, channel, self._stride[0] * c + i,\n",
    "                                      self._stride[1] * d + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width))\n",
    "                              for image in range(images)\n",
    "                              for channel in range(channels)]) + self._bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling in 4 axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n",
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 31, 31)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Subsampling_4d(Layer):\n",
    "    def __init__(self, kernel_size, feature_map, function,\n",
    "                 stride=(1, 1), padding=(0, 0), activation=None,\n",
    "                 bias=0):\n",
    "        # All sizes are expressed as (rows, columns).\n",
    "\n",
    "        #self._error_check(kernel_size, feature_map, stride, padding)\n",
    "\n",
    "        self._kernel_size = kernel_size\n",
    "        self._function = function\n",
    "        self._activation = activation\n",
    "        self._bias = bias\n",
    "\n",
    "        self._stride = stride\n",
    "        self._padding = padding\n",
    "\n",
    "        super().__init__(input_data=feature_map)\n",
    "\n",
    "\n",
    "    def _allocate(self):\n",
    "        map_height = self._input_data.shape[2] + 2 * self._padding[0]\n",
    "        map_width = self._input_data.shape[3] + 2 * self._padding[1]\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        a, b, c, d = dimensions('a b c d')\n",
    "        gridB = Grid(shape=(self._input_data.shape[0], self._input_data.shape[1], map_height, map_width),\\\n",
    "                    dimensions=(a, b, c, d))\n",
    "        B = Function(name='B', grid=gridB, space_order=0)\n",
    "\n",
    "        e, f, g, h = dimensions('e f g h')\n",
    "        gridR = Grid(shape=( self._input_data.shape[0],  self._input_data.shape[1],\\\n",
    "                            (map_height - kernel_height + self._stride[0])\n",
    "                            // self._stride[0],\n",
    "                            (map_width - kernel_width + self._stride[1])\n",
    "                            // self._stride[1]),\n",
    "                     dimensions=(e, f, g, h))\n",
    "        #print(gridR)\n",
    "        R = Function(name='R', grid=gridR, space_order=0)\n",
    "        #add padding to start and end of each row\n",
    "        for image in range(self._input_data.shape[0]):\n",
    "            for channel in range(self._input_data.shape[1]):\n",
    "                for i in range(self._padding[0], map_height - self._padding[0]):\n",
    "                    B.data[image, channel, i] = \\\n",
    "                        np.concatenate(([0] * self._padding[1],\n",
    "                                        self._input_data[image, channel, i - self._padding[0]],\n",
    "                                        [0] * self._padding[1]))\n",
    "\n",
    "        self._B = B\n",
    "        #print(\"b dim\", self._B.dimensions)\n",
    "        #print(\"b self shape\", self._B.shape)\n",
    "        #print(\"b  shape\", B.shape)\n",
    "        return R\n",
    "\n",
    "    def equations(self):\n",
    "        a, b, c, d = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        images = self._input_data.shape[0]\n",
    "        channels = self._input_data.shape[1] \n",
    "        \n",
    "\n",
    "        rhs = self._function(self._B[image, channel, self._stride[0] * c + i,\n",
    "                                              self._stride[1] * d + j]\n",
    "                                      for image in range(images)\n",
    "                                      for channel in range(channels)\n",
    "                                      for i in range(kernel_height)\n",
    "                                      for j in range(kernel_width)\n",
    "                                      ) + self._bias\n",
    "\n",
    "\n",
    "        \n",
    "        if self._activation is not None:\n",
    "            rhs = self._activation(rhs)\n",
    "\n",
    "        return [Eq(self._R[a, b, c, d], rhs)]\n",
    "\n",
    "Sample_obj4 = Subsampling_4d((2,2),test_tensor, lambda l: sympy.Max(*l))\n",
    "#print(Sample_obj4.equations())\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          ...,\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
       "          [  1.,   1.,   1.,  ...,   1.,   1.,   1.]],\n",
       "\n",
       "         [[ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "          [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "          [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "          ...,\n",
       "          [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "          [ 10.,  10.,  10.,  ...,  10.,  10.,  10.],\n",
       "          [ 10.,  10.,  10.,  ...,  10.,  10.,  10.]],\n",
       "\n",
       "         [[100., 100., 100.,  ..., 100., 100., 100.],\n",
       "          [100., 100., 100.,  ..., 100., 100., 100.],\n",
       "          [100., 100., 100.,  ..., 100., 100., 100.],\n",
       "          ...,\n",
       "          [100., 100., 100.,  ..., 100., 100., 100.],\n",
       "          [100., 100., 100.,  ..., 100., 100., 100.],\n",
       "          [100., 100., 100.,  ..., 100., 100., 100.]]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.ones_like(single_image)\n",
    "test_tensor[0][1][:][:] = 10\n",
    "test_tensor[0][2][:][:] = 100\n",
    "for image in range(test_tensor.shape[0]):\n",
    "    for channel in range(test_tensor.shape[1]):\n",
    "        for h in range(test_tensor.shape[2]):\n",
    "            for w in range(test_tensor.shape[3]):\n",
    "                test_tensor[image][channel][h][w] = test_tensor[image][channel][h][w] + h + w\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0), shape=(31, 31), dimensions=(a, b)]\n",
      "b shape (32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31, 31)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the results\n",
    "Sample_obj = Subsampling((2,2),single_image[0][0], lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tupC = Sample_obj.execute()\n",
    "tupC[0].apply()\n",
    "tupC[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(1, 3, 31, 31), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (1, 3, 32, 32)\n",
      "b  shape (1, 3, 32, 32)\n",
      "[Eq(R[a, b, c, d], Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1], B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1], B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 31, 31)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the 4d\n",
    "Sample_obj4 = Subsampling_4d((2,2),single_image, lambda l: sympy.Max(*l))\n",
    "print(Sample_obj4.equations())\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
       "      31, 31, 31, 31, 31, 31, 31, 31, 29, 27, 28, 29, 31, 31])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tup4[1][0][0] == tupC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Eq(R[a, b, c, d], Max(B[0, 2, a, b], B[0, 2, a, b + 1], B[0, 2, a + 1, b], B[0, 2, a + 1, b + 1]))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj4.equations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = ([self._function([self._B[image, channel, self._stride[0] * c + i,\n",
    "                                      self._stride[1] * d + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width)])\n",
    "                              for image in range(images)\n",
    "                              for channel in range(channels)\n",
    "                              ]) + self._bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'op' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-40703d8d77f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'op' is not defined"
     ]
    }
   ],
   "source": [
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each channel has the same values\n",
    "np.allclose(tup4[1][0][0],tup4[1][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the convolution. It is just an activation function for the Subsampling class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(1, 3, 30, 30), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (1, 3, 32, 32)\n",
      "b  shape (1, 3, 32, 32)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 30, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv_4d(Layer):\n",
    "    def __init__(self, kernel, input_data, stride=(1, 1), padding=(0, 0),\n",
    "                 activation=None, bias=0):\n",
    "        #self._error_check(kernel, input_data, stride, padding)\n",
    "        #kernels are tensors of size:\n",
    "        #(out_channels, in_channels, kernel_height, kernel_width )\n",
    "        self._kernel = kernel\n",
    "        #self._input_data = input_data\n",
    "        super().__init__(input_data=input_data)\n",
    "        self._layer = Subsampling_4d(kernel_size=(self._kernel.shape[2], self._kernel.shape[3]),\n",
    "                                  feature_map=input_data,\n",
    "                                  function=self._convolve,\n",
    "                                  stride=stride,\n",
    "                                  padding=padding,\n",
    "                                  activation=activation,\n",
    "                                  bias=bias)\n",
    "\n",
    "    def _convolve(self, values):\n",
    "        kernel_size = (self._kernel.shape[0], self._kernel.shape[1], self._kernel.shape[2], self._kernel.shape[3])\n",
    "        #Input data: (batch_size, num_input_channels, image_height, image_width)\n",
    "        acc = 0\n",
    "        images = self._input_data.shape[0]\n",
    "        out_channels = self._kernel.shape[0]\n",
    "        #print(\"values\",len(values[0]))\n",
    "        for image in range(images):\n",
    "            for channel in range(out_channels):\n",
    "                for i in range(kernel_size[0]):\n",
    "                    for j in range(kernel_size[1]):\n",
    "                        acc += self._kernel[image][channel][i][j] * values[i * kernel_size[0] + j]\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def _allocate(self):\n",
    "        pass\n",
    "\n",
    "    def execute(self):\n",
    "        return self._layer.execute()\n",
    "\n",
    "    def equations(self):\n",
    "        return self._layer.equations()\n",
    "    \n",
    "A = Conv_4d(kernel_3d,single_image)\n",
    "#A.equations()\n",
    "tup = A.execute()\n",
    "tup[0].apply()\n",
    "tup[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_images = small_tensor[0:2]\n",
    "two_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(2, 3, 31, 31), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (2, 3, 32, 32)\n",
      "b  shape (2, 3, 32, 32)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3, 31, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj4 = Subsampling_4d((2,2),two_images, lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(tup4[1][0], tup4[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(1, 3, 31, 31), dimensions=(e, f, g, h)]\n",
      "rhs: [Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1]), Max(B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1]), Max(B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1])]\n",
      "type: <class 'list'>\n"
     ]
    },
    {
     "ename": "SympifyError",
     "evalue": "SympifyError: [Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1]), Max(B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1]), Max(B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1])]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSympifyError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-c4dc95aa16fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mSample_obj4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubsampling_4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#A.equations()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mtup4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSample_obj4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mtup4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtup4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-571a491d1a85>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-212-c4dc95aa16fa>\u001b[0m in \u001b[0;36mequations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_R\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mSample_obj4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubsampling_4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/devito/types/equation.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, lhs, rhs, subdomain, coefficients, implicit_dims, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 **kwargs):\n\u001b[1;32m     66\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubdomain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_substitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/sympy/core/relational.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, lhs, rhs, **options)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mlhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sympify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sympify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mevaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_evaluate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/sympy/core/sympify.py\u001b[0m in \u001b[0;36m_sympify\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \"\"\"\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/sympy/core/sympify.py\u001b[0m in \u001b[0;36msympify\u001b[0;34m(a, locals, convert_xor, strict, rational, evaluate)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSympifyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSympifyError\u001b[0m: SympifyError: [Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1]), Max(B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1]), Max(B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1])]"
     ]
    }
   ],
   "source": [
    "class Subsampling_4d(Layer):\n",
    "    def __init__(self, kernel_size, feature_map, function,\n",
    "                 stride=(1, 1), padding=(0, 0), activation=None,\n",
    "                 bias=0):\n",
    "        # All sizes are expressed as (rows, columns).\n",
    "\n",
    "        #self._error_check(kernel_size, feature_map, stride, padding)\n",
    "\n",
    "        self._kernel_size = kernel_size\n",
    "        self._function = function\n",
    "        self._activation = activation\n",
    "        self._bias = bias\n",
    "\n",
    "        self._stride = stride\n",
    "        self._padding = padding\n",
    "\n",
    "        super().__init__(input_data=feature_map)\n",
    "\n",
    "\n",
    "    def _allocate(self):\n",
    "        map_height = self._input_data.shape[2] + 2 * self._padding[0]\n",
    "        map_width = self._input_data.shape[3] + 2 * self._padding[1]\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        a, b, c, d = dimensions('a b c d')\n",
    "        gridB = Grid(shape=(self._input_data.shape[0], self._input_data.shape[1], map_height, map_width),\\\n",
    "                    dimensions=(a, b, c, d))\n",
    "        B = Function(name='B', grid=gridB, space_order=0)\n",
    "\n",
    "        e, f, g, h = dimensions('e f g h')\n",
    "        gridR = Grid(shape=( self._input_data.shape[0],  self._input_data.shape[1],\\\n",
    "                            (map_height - kernel_height + self._stride[0])\n",
    "                            // self._stride[0],\n",
    "                            (map_width - kernel_width + self._stride[1])\n",
    "                            // self._stride[1]),\n",
    "                     dimensions=(e, f, g, h))\n",
    "        print(gridR)\n",
    "        R = Function(name='R', grid=gridR, space_order=0)\n",
    "        #add padding to start and end of each row\n",
    "        for image in range(self._input_data.shape[0]):\n",
    "            for channel in range(self._input_data.shape[1]):\n",
    "                for i in range(self._padding[0], map_height - self._padding[0]):\n",
    "                    B.data[image, channel, i] = \\\n",
    "                        np.concatenate(([0] * self._padding[1],\n",
    "                                        self._input_data[image, channel, i - self._padding[0]],\n",
    "                                        [0] * self._padding[1]))\n",
    "\n",
    "        self._B = B\n",
    "        return R\n",
    "\n",
    "    def equations(self):\n",
    "        a, b, c, d = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        images = self._input_data.shape[0]\n",
    "        channels = self._input_data.shape[1] \n",
    "        rhs = ([self._function([self._B[image, channel, self._stride[0] * c + i,\n",
    "                                      self._stride[1] * d + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width)])\n",
    "                              for channel in range(channels)\n",
    "                            for image in range(images)])\n",
    "        print(\"rhs:\", rhs)\n",
    "        print(\"type:\", type(rhs))\n",
    "        if self._activation is not None:\n",
    "            rhs = self._activation(rhs)6\n",
    "\n",
    "        return [Eq(self._R[a, b, c, d], rhs)]\n",
    "\n",
    "Sample_obj4 = Subsampling_4d((2,2),single_image, lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape\n",
    "#only the types for which an explicit conversion has been defined are converted.\n",
    "#In the other cases, a SympifyError is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we get a generator- the operator has the correct logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = ([self._function(self._B[image, channel, self._stride[0] * c + i,\n",
    "                                      self._stride[1] * d + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width))\n",
    "                              for image in range(images)\n",
    "                              for channel in range(channels)])\n",
    "\"\"\"\n",
    "SympifyError: SympifyError: [Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1]),\n",
    "Max(B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1]), \n",
    "Max(B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1])]\n",
    "type: <class 'list'>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Eq(R[a, b, c, d], Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1], B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1], B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1]))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj4.equations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#define _POSIX_C_SOURCE 200809L\n",
      "#include \"stdlib.h\"\n",
      "#include \"math.h\"\n",
      "#include \"sys/time.h\"\n",
      "#include \"xmmintrin.h\"\n",
      "#include \"pmmintrin.h\"\n",
      "#include \"omp.h\"\n",
      "\n",
      "struct dataobj\n",
      "{\n",
      "  void *restrict data;\n",
      "  int * size;\n",
      "  int * npsize;\n",
      "  int * dsize;\n",
      "  int * hsize;\n",
      "  int * hofs;\n",
      "  int * oofs;\n",
      "} ;\n",
      "\n",
      "struct profiler\n",
      "{\n",
      "  double section0;\n",
      "} ;\n",
      "\n",
      "\n",
      "int Kernel(struct dataobj *restrict B_vec, struct dataobj *restrict R_vec, const int a_M, const int a_m, const int b_M, const int b_m, const int c_M, const int c_m, const int d_M, const int d_m, struct profiler * timers, const int nthreads_nonaffine)\n",
      "{\n",
      "  float (*restrict B)[B_vec->size[1]][B_vec->size[2]][B_vec->size[3]] __attribute__ ((aligned (64))) = (float (*)[B_vec->size[1]][B_vec->size[2]][B_vec->size[3]]) B_vec->data;\n",
      "  float (*restrict R)[R_vec->size[1]][R_vec->size[2]][R_vec->size[3]] __attribute__ ((aligned (64))) = (float (*)[R_vec->size[1]][R_vec->size[2]][R_vec->size[3]]) R_vec->data;\n",
      "\n",
      "  /* Flush denormal numbers to zero in hardware */\n",
      "  _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON);\n",
      "  _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON);\n",
      "  struct timeval start_section0, end_section0;\n",
      "  gettimeofday(&start_section0, NULL);\n",
      "  /* Begin section0 */\n",
      "  #pragma omp parallel num_threads(nthreads_nonaffine)\n",
      "  {\n",
      "    int chunk_size = (int)(fmax(1, (1.0F/3.0F)*(a_M - a_m + 1)/nthreads_nonaffine));\n",
      "    #pragma omp for collapse(1) schedule(dynamic,chunk_size)\n",
      "    for (int a = a_m; a <= a_M; a += 1)\n",
      "    {\n",
      "      for (int b = b_m; b <= b_M; b += 1)\n",
      "      {\n",
      "        for (int c = c_m; c <= c_M; c += 1)\n",
      "        {\n",
      "          #pragma omp simd aligned(B,R:32)\n",
      "          for (int d = d_m; d <= d_M; d += 1)\n",
      "          {\n",
      "            R[a][b][c][d] = fmax(B[0][0][c][d], fmax(B[0][1][c][d], fmax(B[0][2][c][d], fmax(B[0][0][c][d + 1], fmax(B[0][0][c + 1][d], fmax(B[0][1][c][d + 1], fmax(B[0][1][c + 1][d], fmax(B[0][2][c][d + 1], fmax(B[0][2][c + 1][d], fmax(B[0][0][c + 1][d + 1], fmax(B[0][1][c + 1][d + 1], B[0][2][c + 1][d + 1])))))))))));\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  /* End section0 */\n",
      "  gettimeofday(&end_section0, NULL);\n",
      "  timers->section0 += (double)(end_section0.tv_sec-start_section0.tv_sec)+(double)(end_section0.tv_usec-start_section0.tv_usec)/1000000;\n",
      "  return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tup4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Eq(R[a, b, c, d], Max(B[0, 0, c, d], B[0, 0, c, d + 1], B[0, 0, c + 1, d], B[0, 0, c + 1, d + 1], B[0, 1, c, d], B[0, 1, c, d + 1], B[0, 1, c + 1, d], B[0, 1, c + 1, d + 1], B[0, 2, c, d], B[0, 2, c, d + 1], B[0, 2, c + 1, d], B[0, 2, c + 1, d + 1]))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj4.equations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.values>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
