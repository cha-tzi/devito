{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opperations with 4-axes tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/convolution_graph.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning the filter needs to have the same amount of channels as the input image. Each filter will result in one output channel. In other words, when a kernel is applied to an RGB (or an image with Multiple channels) then effectively sum up the output matrices (along with a bias terms) to yield a single-channel feature map. The number of channels of the output image will be equal to the number of filters applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch works on input tensors whose shape correspands to :\n",
    "\n",
    "**(batch_size,  num_input_channels,  image_height,  image_width)**\n",
    "\n",
    "Kernels must be tensors of size: \n",
    "\n",
    "\n",
    "**(out_channels,  in_channels,  kernel_height,  kernel_width )**\n",
    "\n",
    "out_channels: number of kernels, in_channels: number of channels in one kernel\n",
    "\n",
    "Arguments of torch.nn.Conv2d\n",
    "\n",
    "**(in_channel,  out_channel,  ker_size)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz/cifar-10-python.tar.gz to C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "#do the standard imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ## uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms.\n",
    "    torch.backends.cudnn.enabled   = False  ## does not enable the inbuilt cudnn deep learning library for training neural networks\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available(): ## run on GPUs if available\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "#Use torchvision.datasets.CIFAR10 to load the CIFAR10 dataset\n",
    "cifar10_train = CIFAR10(\"C:/Users/gc2016/OneDrive - Imperial College London/ACSE/ACSE-9/cifar-10-python.tar.gz\", download=True, train=True)\n",
    "#cifar10_test = CIFAR10(\"./\", download=True, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the dataset to a torch tensor\n",
    "X_train = torch.tensor(cifar10_train.data).float()\n",
    "#take only the first 50 images \n",
    "#and reshape to batchsize, channels, height, width\n",
    "small_tensor = X_train[0:50,:].transpose(3,1).transpose(2,3)\n",
    "single_image = small_tensor[0:1]\n",
    "single_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the kernel\n",
    "#out_channels, in_channels, kernel_height, kernel_width )\n",
    "#out_channels: number of kernels, in_channels: number of channels in one kernel\n",
    "SOBEL_VERTICAL = [[1, 0, -1],\n",
    "                  [2, 0, -2],\n",
    "                  [1, 0, -1]]\n",
    "SOBEL_HORIZONTAL = [[1, 2, 1],\n",
    "                    [0, 0, 0],\n",
    "                    [-1, -2, -1]]\n",
    "PREWITT_VERTICAL = [[1, 0, -1],\n",
    "                    [1, 0, -1],\n",
    "                    [1, 0, -1]]\n",
    "single_image = small_tensor[0:1,:]\n",
    "#single image with 4 axes\n",
    "print(single_image.shape)\n",
    "#create a kernel with 4 axes\n",
    "kernel_3d = torch.tensor([SOBEL_VERTICAL, SOBEL_HORIZONTAL, PREWITT_VERTICAL]).float()\n",
    "kernel_3d.unsqueeze_(0)\n",
    "kernel_3d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30, 30])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conop = nn.Conv2d(3,1,3, bias = False)\n",
    "input = single_image\n",
    "#set our own weights\n",
    "conop.weight = torch.nn.Parameter( kernel_3d )\n",
    "output = conop(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devito set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from devito import Operator, Function\n",
    "from numpy import array\n",
    "from devito import Grid, Function, dimensions, Eq, Inc\n",
    "import sympy\n",
    "class Layer(ABC):\n",
    "    def __init__(self, input_data):\n",
    "        self._input_data = input_data\n",
    "        self._R = self._allocate()\n",
    "\n",
    "    @abstractmethod\n",
    "    def _allocate(self) -> Function:\n",
    "        # This method should return a Function object corresponding to\n",
    "        # an output of the layer.\n",
    "        pass\n",
    "\n",
    "    def execute(self) -> (Operator, array):\n",
    "        op = Operator(self.equations())\n",
    "        op.cfunction\n",
    "\n",
    "        return (op, self._R.data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def equations(self) -> list:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Dimentional Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subsampling(Layer):\n",
    "    def __init__(self, kernel_size, feature_map, function,\n",
    "                 stride=(1, 1), padding=(0, 0), activation=None,\n",
    "                 bias=0):\n",
    "        # All sizes are expressed as (rows, columns).\n",
    "\n",
    "        self._error_check(kernel_size, feature_map, stride, padding)\n",
    "\n",
    "        self._kernel_size = kernel_size\n",
    "        self._function = function\n",
    "        self._activation = activation\n",
    "        self._bias = bias\n",
    "\n",
    "        self._stride = stride\n",
    "        self._padding = padding\n",
    "\n",
    "        super().__init__(input_data=feature_map)\n",
    "\n",
    "    def _error_check(self, kernel_size, feature_map, stride, padding):\n",
    "        if feature_map is None or len(feature_map) == 0:\n",
    "            raise Exception(\"Feature map must not be empty\")\n",
    "\n",
    "        if kernel_size is None or len(kernel_size) != 2:\n",
    "            raise Exception(\"Kernel size is incorrect\")\n",
    "\n",
    "        if stride is None or len(stride) != 2:\n",
    "            raise Exception(\"Stride is incorrect\")\n",
    "\n",
    "        if stride[0] < 1 or stride[1] < 1:\n",
    "            raise Exception(\"Stride cannot be less than 1\")\n",
    "\n",
    "        if padding is None or len(padding) != 2:\n",
    "            raise Exception(\"Padding is incorrect\")\n",
    "\n",
    "        if padding[0] < 0 or padding[1] < 0:\n",
    "            raise Exception(\"Padding cannot be negative\")\n",
    "\n",
    "        map_height = len(feature_map) + 2 * padding[0]\n",
    "        map_width = len(feature_map[0]) + 2 * padding[1]\n",
    "        kernel_height, kernel_width = kernel_size\n",
    "\n",
    "        if (map_height - kernel_height) % stride[0] != 0 or \\\n",
    "           (map_width - kernel_width) % stride[1] != 0:\n",
    "            raise Exception(\"Stride \" + str(stride) + \" is not \"\n",
    "                            \"compatible with feature map, kernel and padding \"\n",
    "                            \"sizes\")\n",
    "\n",
    "    def _allocate(self):\n",
    "        map_height = len(self._input_data) + 2 * self._padding[0]\n",
    "        map_width = len(self._input_data[0]) + 2 * self._padding[1]\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "\n",
    "        gridB = Grid(shape=(map_height, map_width))\n",
    "        B = Function(name='B', grid=gridB, space_order=0)\n",
    "\n",
    "        a, b = dimensions('a b')\n",
    "        gridR = Grid(shape=((map_height - kernel_height + self._stride[0])\n",
    "                            // self._stride[0],\n",
    "                            (map_width - kernel_width + self._stride[1])\n",
    "                            // self._stride[1]),\n",
    "                     dimensions=(a, b))\n",
    "        print(gridR)\n",
    "        R = Function(name='R', grid=gridR, space_order=0)\n",
    "\n",
    "        for i in range(self._padding[0], map_height - self._padding[0]):\n",
    "            B.data[i] = \\\n",
    "                np.concatenate(([0] * self._padding[1],\n",
    "                                self._input_data[i - self._padding[0]],\n",
    "                                [0] * self._padding[1]))\n",
    "\n",
    "        self._B = B\n",
    "        print(\"b shape\", B.shape)\n",
    "        return R\n",
    "\n",
    "    def equations(self):\n",
    "        a, b = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "\n",
    "        rhs = self._function([self._B[self._stride[0] * a + i,\n",
    "                                      self._stride[1] * b + j]\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width)]) + self._bias\n",
    "\n",
    "        if self._activation is not None:\n",
    "            rhs = self._activation(rhs)\n",
    "\n",
    "        return [Eq(self._R[a, b], rhs)]\n",
    "                \n",
    "Sample_obj = Subsampling((2,2),single_image[0][0], lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tupC = Sample_obj.execute()\n",
    "tupC[0].apply()\n",
    "tupC[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsampling in 4 axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(1, 3, 31, 31), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (1, 3, 32, 32)\n",
      "b  shape (1, 3, 32, 32)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 31, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Subsampling_4d(Layer):\n",
    "    def __init__(self, kernel_size, feature_map, function,\n",
    "                 stride=(1, 1), padding=(0, 0), activation=None,\n",
    "                 bias=0):\n",
    "        # All sizes are expressed as (rows, columns).\n",
    "\n",
    "        #self._error_check(kernel_size, feature_map, stride, padding)\n",
    "\n",
    "        self._kernel_size = kernel_size\n",
    "        self._function = function\n",
    "        self._activation = activation\n",
    "        self._bias = bias\n",
    "\n",
    "        self._stride = stride\n",
    "        self._padding = padding\n",
    "\n",
    "        super().__init__(input_data=feature_map)\n",
    "\n",
    "\n",
    "    def _allocate(self):\n",
    "        map_height = self._input_data.shape[2] + 2 * self._padding[0]\n",
    "        map_width = self._input_data.shape[3] + 2 * self._padding[1]\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        a, b, c, d = dimensions('a b c d')\n",
    "        gridB = Grid(shape=(self._input_data.shape[0], self._input_data.shape[1], map_height, map_width),\\\n",
    "                    dimensions=(a, b, c, d))\n",
    "        B = Function(name='B', grid=gridB, space_order=0)\n",
    "\n",
    "        e, f, g, h = dimensions('e f g h')\n",
    "        gridR = Grid(shape=( self._input_data.shape[0],  self._input_data.shape[1],\\\n",
    "                            (map_height - kernel_height + self._stride[0])\n",
    "                            // self._stride[0],\n",
    "                            (map_width - kernel_width + self._stride[1])\n",
    "                            // self._stride[1]),\n",
    "                     dimensions=(e, f, g, h))\n",
    "        print(gridR)\n",
    "        R = Function(name='R', grid=gridR, space_order=0)\n",
    "        #add padding to start and end of each row\n",
    "        for image in range(self._input_data.shape[0]):\n",
    "            for channel in range(self._input_data.shape[1]):\n",
    "                for i in range(self._padding[0], map_height - self._padding[0]):\n",
    "                    B.data[image, channel, i] = \\\n",
    "                        np.concatenate(([0] * self._padding[1],\n",
    "                                        self._input_data[image, channel, i - self._padding[0]],\n",
    "                                        [0] * self._padding[1]))\n",
    "\n",
    "        self._B = B\n",
    "        print(\"b dim\", self._B.dimensions)\n",
    "        print(\"b self shape\", self._B.shape)\n",
    "        print(\"b  shape\", B.shape)\n",
    "        return R\n",
    "\n",
    "    def equations(self):\n",
    "        a, b, c, d = self._B.dimensions\n",
    "        kernel_height, kernel_width = self._kernel_size\n",
    "        images = self._input_data.shape[0]\n",
    "        channels = self._input_data.shape[1] \n",
    "        print(channels)\n",
    "        rhs = self._function([self._B[image, channel, self._stride[0] * c + i,\n",
    "                                      self._stride[1] * d + j]\n",
    "                              for image in range(images)\n",
    "                              for channel in range(channels)\n",
    "                              for i in range(kernel_height)\n",
    "                              for j in range(kernel_width)\n",
    "                              ]) + self._bias\n",
    "\n",
    "        if self._activation is not None:\n",
    "            rhs = self._activation(rhs)\n",
    "\n",
    "        return [Eq(self._R[a, b, c, d], rhs)]\n",
    "\n",
    "Sample_obj4 = Subsampling_4d((2,2),single_image, lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each channel has the same values\n",
    "np.allclose(tup4[1][0][0],tup4[1][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the convolution. It is just an activation function for the Subsampling class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(1, 3, 30, 30), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (1, 3, 32, 32)\n",
      "b  shape (1, 3, 32, 32)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 30, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv_4d(Layer):\n",
    "    def __init__(self, kernel, input_data, stride=(1, 1), padding=(0, 0),\n",
    "                 activation=None, bias=0):\n",
    "        #self._error_check(kernel, input_data, stride, padding)\n",
    "        #kernels are tensors of size:\n",
    "        #(out_channels, in_channels, kernel_height, kernel_width )\n",
    "        self._kernel = kernel\n",
    "        #self._input_data = input_data\n",
    "        super().__init__(input_data=input_data)\n",
    "        self._layer = Subsampling_4d(kernel_size=(self._kernel.shape[2], self._kernel.shape[3]),\n",
    "                                  feature_map=input_data,\n",
    "                                  function=self._convolve,\n",
    "                                  stride=stride,\n",
    "                                  padding=padding,\n",
    "                                  activation=activation,\n",
    "                                  bias=bias)\n",
    "\n",
    "    def _convolve(self, values):\n",
    "        kernel_size = (self._kernel.shape[0], self._kernel.shape[1], self._kernel.shape[2], self._kernel.shape[3])\n",
    "        #Input data: (batch_size, num_input_channels, image_height, image_width)\n",
    "        acc = 0\n",
    "        images = self._input_data.shape[0]\n",
    "        out_channels = self._kernel.shape[0]\n",
    "        #print(\"values\",len(values[0]))\n",
    "        for image in range(images):\n",
    "            for channel in range(out_channels):\n",
    "                for i in range(kernel_size[0]):\n",
    "                    for j in range(kernel_size[1]):\n",
    "                        acc += self._kernel[image][channel][i][j] * values[i * kernel_size[0] + j]\n",
    "\n",
    "        return acc\n",
    "\n",
    "\n",
    "    def _allocate(self):\n",
    "        pass\n",
    "\n",
    "    def execute(self):\n",
    "        return self._layer.execute()\n",
    "\n",
    "    def equations(self):\n",
    "        return self._layer.equations()\n",
    "    \n",
    "A = Conv_4d(kernel_3d,single_image)\n",
    "#A.equations()\n",
    "tup = A.execute()\n",
    "tup[0].apply()\n",
    "tup[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_images = small_tensor[0:2]\n",
    "two_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid[extent=(1.0, 1.0, 1.0, 1.0), shape=(2, 3, 31, 31), dimensions=(e, f, g, h)]\n",
      "b dim (a, b, c, d)\n",
      "b self shape (2, 3, 32, 32)\n",
      "b  shape (2, 3, 32, 32)\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operator `Kernel` run in 0.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3, 31, 31)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sample_obj4 = Subsampling_4d((2,2),two_images, lambda l: sympy.Max(*l))\n",
    "#A.equations()\n",
    "tup4 = Sample_obj4.execute()\n",
    "tup4[0].apply()\n",
    "tup4[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(tup4[1][0], tup4[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
